{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNets - Base Model\n",
    "\n",
    "To perform transfer learning and fine-tuning, we will first train our base model with EfficientNet as a \"feature extractor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key variables\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sys.path.append('../source')\n",
    "session = Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the metadata from preprocessing.\n",
    "\n",
    "***Note: Make sure you have preprocessed `EfficientNet-b3` data for training the base model.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../data/mit_indoor_67/metadata/'\n",
    "efficientnet = 'efficientnet-b0'  # change to different versions for training base model\n",
    "metadata_file = root_dir + efficientnet.replace(\"-\", \"_\") + \".pkl\"\n",
    "metadata = pickle.load(open(metadata_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output_path, source directory and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artefacts will be saved to: s3://sagemaker-us-east-2-194071253362/mit_indoor_67\n"
     ]
    }
   ],
   "source": [
    "prefix = 'mit_indoor_67'\n",
    "output_path = os.path.join('s3://', bucket, prefix)\n",
    "print('model artefacts will be saved to: {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source directory in training\n",
    "source_dir = '../source'\n",
    "dependencies = ['../source/dataset', '../source/utils']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script:\n",
    "\n",
    "*Note: Except optimizer selection, Dataloading, Model selection, and training are all wrapped in helper classes/functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcopy\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Image\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msix\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BytesIO\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataLoader\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdataset\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mENindoor67\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Composer, ENindoor67Preprocessed\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mhelpers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_dataloader, get_hyperparameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtrainer\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Trainer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModelMaker\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36madabound\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AdaBound\r\n",
      "\r\n",
      "\u001b[37m# Random seeds\u001b[39;49;00m\r\n",
      "torch.manual_seed(\u001b[34m1\u001b[39;49;00m)\r\n",
      "torch.cuda.manual_seed(\u001b[34m1\u001b[39;49;00m)\r\n",
      "np.random.seed(\u001b[34m1\u001b[39;49;00m)\r\n",
      "random.seed(\u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n",
      "    model_info = {}\r\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = torch.load(f)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
      "\r\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Instantiate the model \u001b[39;49;00m\r\n",
      "    model, _ = ModelMaker().make_model(**model_info)\r\n",
      "    \r\n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "\r\n",
      "    \u001b[37m# set to eval mode, could use no_grad\u001b[39;49;00m\r\n",
      "    model.to(device).eval()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_weights\u001b[39;49;00m():\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.listdir(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/base/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnzipping base model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    subprocess.check_call([\u001b[33m'\u001b[39;49;00m\u001b[33mtar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33m-xvf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/base/model.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    subprocess.check_call([\u001b[33m'\u001b[39;49;00m\u001b[33mrm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-rf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/base/model.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    model_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        state_dict = torch.load(f)\r\n",
      "    \r\n",
      "    model_info = {}\r\n",
      "    model_info_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m summaryf:\r\n",
      "        model_info = torch.load(summaryf)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded base model: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m state_dict\r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    \r\n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments\u001b[39;49;00m\r\n",
      "    \u001b[37m# when this script is executed, during a training job\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[37m# Here we set up an argument parser to easily access the parameters\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# SageMaker parameters, like the directories for training data and saving models; set automatically\u001b[39;49;00m\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-metadata-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--val-metadata-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VAL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--base-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_BASE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mKeyError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mNo base model weights found.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--config\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, choices=[\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33maugmented\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33moriginal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mCONF\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mdataset configuration (default: None)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sampling\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mweighted\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, choices=[\u001b[33m'\u001b[39;49;00m\u001b[33mweighted\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msubsetrandom\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mSM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33msampling method for train set (default : `weighted`)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.001)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--final-lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mFLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mfinal learning rate for AdaBound (default: 0.1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mAdam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mOPTIM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33moptimizer used in training (default: `Adam`)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mMTM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mmomentum factor used in SGD (default: 0.9)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mResNeXt101\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mpretrained model for training (default: `ResNeXt101`)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mW\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of workers (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--patience\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mP\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs for no improvments in val loss for early stopping. (default: 3)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-classes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m67\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mNC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of predicted classes (default: 67)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--blocks-unfrozen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLU\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of layers to unfreeze in EfficientNet (default:2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mDO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mdropout rate for fine-tuning (default: 0.2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mDP\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mdepth of tuned model (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--weight-decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mWD\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mweight decay rate / L2 regularization to Adam / AdamW (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    \u001b[37m# args holds all passed-in arguments\u001b[39;49;00m\r\n",
      "    args = parser.parse_args()\r\n",
      "\r\n",
      "    \r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\r\n",
      "\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "    torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "    \u001b[37m# Load the training and validation data.\u001b[39;49;00m\r\n",
      "    train_loader = get_dataloader(args.train_metadata_dir,\r\n",
      "                                  args.config,\r\n",
      "                                  batch_size=args.batch_size,\r\n",
      "                                  method=args.sampling,\r\n",
      "                                  random_state=args.seed,\r\n",
      "                                  subset=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                  workers=args.workers)\r\n",
      "    val_loader = get_dataloader(args.val_metadata_dir,\r\n",
      "                                args.config,\r\n",
      "                                batch_size=args.batch_size,\r\n",
      "                                subset=\u001b[33m'\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                random_state=args.seed,\r\n",
      "                                workers=args.workers)\r\n",
      "    \r\n",
      "    \u001b[37m#instantiate the model\u001b[39;49;00m\r\n",
      "    model, model_info = ModelMaker().make_model(model=args.model,\r\n",
      "                                                num_classes=args.num_classes,\r\n",
      "                                                blocks_unfrozen=args.blocks_unfrozen,\r\n",
      "                                                dropout=args.dropout,\r\n",
      "                                                depth=args.depth)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m loaded.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model))\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_BASE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m os.environ:\r\n",
      "        model.load_state_dict(load_weights())\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded base model weights.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \r\n",
      "    \u001b[37m# optimizer\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = optim.SGD(model.parameters(),\r\n",
      "                              lr=args.lr,\r\n",
      "                              momentum=args.momentum)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = optim.Adam(model.parameters(),\r\n",
      "                               lr=args.lr,\r\n",
      "                               amsgrad=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                               weight_decay=args.weight_decay)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33mamsgrad\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = optim.Adam(model.parameters(),\r\n",
      "                               lr=args.lr,\r\n",
      "                               amsgrad=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                               weight_decay=args.weight_decay)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33madabound\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = AdaBound(model.parameters(),\r\n",
      "                             lr=args.lr,\r\n",
      "                             final_lr=args.final_lr)\r\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33madamw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        optimizer = optim.AdamW(model.parameters(),\r\n",
      "                                lr=args.lr,\r\n",
      "                                amsgrad=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                weight_decay=args.weight_decay)\r\n",
      "        \r\n",
      "    criterion = nn.CrossEntropyLoss()\r\n",
      "    \r\n",
      "    trainer = Trainer()\r\n",
      "    model, summary = trainer.run(model=model,\r\n",
      "                                 train_loader=train_loader,\r\n",
      "                                 val_loader=val_loader,\r\n",
      "                                 epochs=args.epochs,\r\n",
      "                                 criterion=criterion,\r\n",
      "                                 optimizer=optimizer,\r\n",
      "                                 device=device,\r\n",
      "                                 patience=args.patience)\r\n",
      "    \r\n",
      "    summary[\u001b[33m'\u001b[39;49;00m\u001b[33mhyperparameters\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = get_hyperparameters(args)\r\n",
      "    \r\n",
      "    \r\n",
      "    \u001b[37m# Save the training summary \u001b[39;49;00m\r\n",
      "    summary_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtraining_summary.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(summary_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m summary_file:\r\n",
      "        torch.save(summary, summary_file)\r\n",
      "    \r\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m model_info_file:\r\n",
      "        torch.save(model_info, model_info_file)\r\n",
      "    \r\n",
      "    \u001b[37m# Save the model parameters\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        torch.save(model.cpu().state_dict(), f)\r\n",
      "\r\n",
      "        \r\n",
      "        \r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ../source/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mitindoor67-efficientnet-b0-base-2020-12-10-03-30-18\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "# Change model name if replicating final model\n",
    "job_name = \"mitindoor67-{}-base-{}\".format(efficientnet, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "# job_name = \"mitindoor67-{}-final-{}\".format(efficientnet, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Set instance details, Pytorch framework version and hyperparameters:\n",
    "\n",
    "We will use an `AdamW` optimizer with default setting: `lr` (learning rate) of 0.001 and `weight_decay` of 0.01, and `subsetrandom` sampling. We will use simple fully connected layer: add a `dropout` of 0.5 and a final classification layer with `num_classes` = 67.\n",
    "\n",
    "To make sure the base model converge and do not overfit, we initially set the model to be trained for 50 epochs with `patience` of 5 - such that if the model is not improving for 5 consecutive epochs, we will implement early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point = 'main.py'  # training script\n",
    "instance_type = 'ml.p3.2xlarge'  # training on ml.p2.xlarge may take hours hence we switch to ml.p3.2xlarge\n",
    "instance_count = 1  # number of instance\n",
    "framework_version = '1.6.0'  # Pytorch version\n",
    "py_version = 'py3'  # Python version\n",
    "\n",
    "# Comment out lines below to train Final Model\n",
    "hyperparameters = {\n",
    "                    'model' : 'EfficientNet-lite0',\n",
    "                    'epochs': 20, # Set to 20 epochs initially\n",
    "                    'batch-size' : 32, \n",
    "                    'sampling' : 'subsetrandom',\n",
    "                    'optimizer' : 'adamw',\n",
    "                    'workers' : 7,  # num_cpu - 1\n",
    "                    'blocks-unfrozen' : 0,\n",
    "                    'patience' : 3,\n",
    "                    'dropout' : 0.5\n",
    "                }  \n",
    "\n",
    "\n",
    "# Uncomment lines below to train Final Model\n",
    "# hyperparameters = {\n",
    "#                     'model' : 'EfficientNet-b3',\n",
    "#                     'epochs': 20, # Set to 20 epochs initially\n",
    "#                     'batch-size' : 32, \n",
    "#                     'sampling' : 'subsetrandom',\n",
    "#                     'optimizer' : 'adamw',\n",
    "#                     'workers' : 7,  # num_cpu - 1\n",
    "#                     'blocks-unfrozen' : 23,\n",
    "#                     'lr' : 1e-4,\n",
    "#                     'weight-decay' : 0.0072299723855532155,\n",
    "#                     'dropout' : 0.7005047299544908,\n",
    "#                     'patience' : 3,\n",
    "#                 }  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# initial attempt\n",
    "estimator = PyTorch(entry_point=entry_point,\n",
    "                    source_dir=source_dir,\n",
    "                    dependencies=dependencies,\n",
    "                    role=role,\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    framework_version=framework_version,\n",
    "                    py_version=py_version,\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=session,\n",
    "                    hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the data\n",
    "\n",
    "Fit the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out the lines below to train final model\n",
    "estimator.fit({\n",
    "    'train': metadata['train'],\n",
    "    'val' : metadata['val']},\n",
    "    job_name=job_name,\n",
    "    wait = False)\n",
    "\n",
    "# Uncomment lines below to replicate training of Final Model\n",
    "# base_model_job = 'mitindoor67-efficientnet-b3-base-2020-11-28-07-08-59' # BASE JOB NAME HERE\n",
    "# base_model = os.path.join(output_path, base_model_job, 'output', 'model.tar.gz')\n",
    "\n",
    "# print(base_model)\n",
    "\n",
    "# estimator.fit({\n",
    "#     'train': metadata['train'],\n",
    "#     'val' : metadata['val'],\n",
    "#     'base' : base_model},\n",
    "#     job_name=job_name,\n",
    "#     wait = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained the base model, we shall begin with hyperparameter tuning for our final model. We will feed in the base model, unfreeze a fraction of layers and [tuning hyperparameters by Bayesian search](./EfficientNets-HPO.ipynb) using AWS Sagemaker's Hyperparameter Tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
