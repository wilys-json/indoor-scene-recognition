{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNets (Hyperparameter Tuning)\n",
    "\n",
    "Here we will explore the optimal hyperparameters in fine-tuning our model.\n",
    "\n",
    "***Note: Make sure you have trained the base model before fine-tuning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sys.path.append('../source')\n",
    "session = Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://sagemaker-us-east-2-194071253362/mit_indoor_67/processed/train/efficientnet_b3',\n",
       " 'val': 's3://sagemaker-us-east-2-194071253362/mit_indoor_67/processed/val/efficientnet_b3',\n",
       " 'test': 's3://sagemaker-us-east-2-194071253362/mit_indoor_67/processed/test/efficientnet_b3/indoor67_test.pkl'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../data/mit_indoor_67/metadata/'\n",
    "efficientnet = 'efficientnet-b3'  # Version of the base model\n",
    "metadata_file = root_dir + efficientnet.replace(\"-\", \"_\") + \".pkl\"\n",
    "metadata = pickle.load(open(metadata_file, 'rb'))\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output_path, source directory and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artefacts will be saved to: s3://sagemaker-us-east-2-194071253362/mit_indoor_67\n"
     ]
    }
   ],
   "source": [
    "prefix = 'mit_indoor_67'\n",
    "output_path = os.path.join('s3://', bucket, prefix)\n",
    "print('model artefacts will be saved to: {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '../source'\n",
    "dependencies = ['../source/dataset', '../source/utils']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcopy\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Image\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msix\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BytesIO\n",
      "\u001b[37m# from torch.utils.data import DataLoader\u001b[39;49;00m\n",
      "\u001b[37m# from dataset.ENindoor67 import ENindoor67Preprocessed, ENindoor67Datasets\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataLoader\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdataset\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mENindoor67\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Composer, ENindoor67Preprocessed\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mhelpers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_dataloader, get_hyperparameters\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtrainer\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Trainer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtester\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m S3Tester\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModelMaker\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36madabound\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AdaBound\n",
      "\n",
      "IMG_CONTENT_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-image\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "PKL_CONTENT_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/python-pickle\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "NP_CONTENT_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Instantiate the model \u001b[39;49;00m\n",
      "    model, _ = ModelMaker().make_model(**model_info)\n",
      "    \n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    \u001b[37m# set to eval mode, could use no_grad\u001b[39;49;00m\n",
      "    model.to(device).eval()\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_weights\u001b[39;49;00m():\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(os.listdir(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/base/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnzipping base model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    subprocess.check_call([\u001b[33m'\u001b[39;49;00m\u001b[33mtar\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33m-xvf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/base/model.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    subprocess.check_call([\u001b[33m'\u001b[39;49;00m\u001b[33mrm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-rf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/base/model.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    model_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        state_dict = torch.load(f)\n",
      "    \n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m summaryf:\n",
      "        model_info = torch.load(summaryf)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded base model: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m state_dict\n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments\u001b[39;49;00m\n",
      "    \u001b[37m# when this script is executed, during a training job\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Here we set up an argument parser to easily access the parameters\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# SageMaker parameters, like the directories for training data and saving models; set automatically\u001b[39;49;00m\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-metadata-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--val-metadata-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VAL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--base-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_BASE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mKeyError\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mNo base model weights found.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--config\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, choices=[\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33maugmented\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33moriginal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mCONF\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mdataset configuration (default: None)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sampling\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mweighted\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, choices=[\u001b[33m'\u001b[39;49;00m\u001b[33mweighted\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msubsetrandom\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mSM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33msampling method for train set (default : `weighted`)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.001)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--final-lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mFLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mfinal learning rate for AdaBound (default: 0.1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mAdam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mOPTIM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33moptimizer used in training (default: `Adam`)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mMTM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mmomentum factor used in SGD (default: 0.9)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mResNeXt101\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mpretrained model for training (default: `ResNeXt101`)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mW\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of workers (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--patience\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mP\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs for no improvments in val loss for early stopping. (default: 3)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-classes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m67\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mNC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of predicted classes (default: 67)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--blocks-unfrozen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLU\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of layers to unfreeze in EfficientNet (default:2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mDO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mdropout rate for fine-tuning (default: 0.2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mDP\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mdepth of tuned model (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--weight-decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mWD\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mweight decay rate / L2 regularization to Adam / AdamW (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \u001b[37m# args holds all passed-in arguments\u001b[39;49;00m\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    \n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    torch.manual_seed(args.seed)\n",
      "    torch.cuda.manual_seed(args.seed)\n",
      "\n",
      "    \u001b[37m# Load the training and validation data.\u001b[39;49;00m\n",
      "    train_loader = get_dataloader(args.train_metadata_dir,\n",
      "                                  args.config,\n",
      "                                  batch_size=args.batch_size,\n",
      "                                  method=args.sampling,\n",
      "                                  random_state=args.seed,\n",
      "                                  subset=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                  workers=args.workers)\n",
      "    val_loader = get_dataloader(args.val_metadata_dir,\n",
      "                                args.config,\n",
      "                                batch_size=args.batch_size,\n",
      "                                subset=\u001b[33m'\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                random_state=args.seed,\n",
      "                                workers=args.workers)\n",
      "    \n",
      "    \u001b[37m#instantiate the model\u001b[39;49;00m\n",
      "    model, model_info = ModelMaker().make_model(model=args.model,\n",
      "                                              num_classes=args.num_classes,\n",
      "                                              blocks_unfrozen=args.blocks_unfrozen,\n",
      "                                              dropout=args.dropout,\n",
      "                                              depth=args.depth)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m loaded.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model))\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_BASE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m os.environ:\n",
      "        model.load_state_dict(load_weights())\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded base model weights.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "    \u001b[37m# optimizer\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        optimizer = optim.SGD(model.parameters(),\n",
      "                              lr=args.lr,\n",
      "                              momentum=args.momentum)\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        optimizer = optim.Adam(model.parameters(),\n",
      "                               lr=args.lr,\n",
      "                               amsgrad=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                               weight_decay=args.weight_decay)\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33mamsgrad\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        optimizer = optim.Adam(model.parameters(),\n",
      "                               lr=args.lr,\n",
      "                               amsgrad=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                               weight_decay=args.weight_decay)\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33madabound\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        optimizer = AdaBound(model.parameters(),\n",
      "                             lr=args.lr,\n",
      "                             final_lr=args.final_lr)\n",
      "    \u001b[34melif\u001b[39;49;00m args.optimizer.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33madamw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        optimizer = optim.AdamW(model.parameters(),\n",
      "                                lr=args.lr,\n",
      "                                amsgrad=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                                weight_decay=args.weight_decay)\n",
      "        \n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    \n",
      "    trainer = Trainer()\n",
      "    model, summary = trainer.run(model=model,\n",
      "                                 train_loader=train_loader,\n",
      "                                 val_loader=val_loader,\n",
      "                                 epochs=args.epochs,\n",
      "                                 criterion=criterion,\n",
      "                                 optimizer=optimizer,\n",
      "                                 device=device,\n",
      "                                 patience=args.patience)\n",
      "    \n",
      "    summary[\u001b[33m'\u001b[39;49;00m\u001b[33mhyperparameters\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = get_hyperparameters(args)\n",
      "    \n",
      "    \n",
      "    \u001b[37m# Save the training summary \u001b[39;49;00m\n",
      "    summary_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtraining_summary.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(summary_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m summary_file:\n",
      "        torch.save(summary, summary_file)\n",
      "    \n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m model_info_file:\n",
      "        torch.save(model_info, model_info_file)\n",
      "    \n",
      "\t\u001b[37m# Save the model parameters\u001b[39;49;00m\n",
      "    model_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model.cpu().state_dict(), f)\n",
      "\n",
      "        \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "!pygmentize ../source/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-b3-hpo-1128-125011\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "job_name = \"{}-hpo-{}\".format(efficientnet, strftime(\"%m%d-%H%M%S\", gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set instance details, Pytorch framework version and hyperparameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.p3.2xlarge'\n",
    "instance_count = 1\n",
    "framework_version = '1.6.0'\n",
    "entry_point='main.py'\n",
    "py_version='py3'\n",
    "hyperparameters = {\n",
    "                    'model' : 'EfficientNet-b3',\n",
    "                    'epochs': 20, # since AdamW converges quickly we only set max. epoch to 20\n",
    "                    'batch-size' : 32,\n",
    "                    'blocks-unfrozen' : 0, \n",
    "                    'sampling' : 'subsetrandom',\n",
    "                    'lr' : 1e-4,\n",
    "                    'workers' : 7,\n",
    "                    'optimizer' : 'adamw',\n",
    "                    'dropout' : 0.5,\n",
    "                    'weight-decay' : 0.01,\n",
    "                    'patience' : 3  # set lower patience to implement early stopping\n",
    "                    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# initial attempt\n",
    "estimator = PyTorch(entry_point=entry_point,\n",
    "                    source_dir=source_dir,\n",
    "                    dependencies=dependencies,\n",
    "                    role=role,\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    framework_version=framework_version,\n",
    "                    py_version=py_version,\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=session,\n",
    "                    hyperparameters=hyperparameters\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Hyperparameter Tuner\n",
    "\n",
    "Here we were unsure how many layers should be unfrozen, as well as the learning rate, weight decay (L2 penalty), dropout. Hence we will use SageMaker's Hyperparameter Tuner to find the optimal settings. The objective was to minimize cross entropy loss in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "                         'blocks-unfrozen' : IntegerParameter(0, 26),\n",
    "                         'weight-decay' : ContinuousParameter(1e-4, 1e-1),\n",
    "                         'dropout' : ContinuousParameter(0.1, 0.9),\n",
    "                         'lr' : ContinuousParameter(1e-4, 1e-1)\n",
    "                         }\n",
    "max_jobs=8\n",
    "max_parallel_jobs=1\n",
    "objective_metric_name = 'Validation Loss'\n",
    "objective_type = 'Minimize'  # Objective\n",
    "metric_definitions = [{'Name': 'Validation Loss',\n",
    "                       'Regex': 'MIN VAL LOSS: ([0-9\\\\.]+)'}]  # Minimum Validation Loss in a particular training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=max_jobs,\n",
    "                            max_parallel_jobs=max_parallel_jobs,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-194071253362/mit_indoor_67/mitindoor67-efficientnet-b3-base-2020-11-28-07-08-59/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# train on a the base mode - feature extractor\n",
    "base_model_job = '' # BASE JOB NAME HERE\n",
    "base_model = os.path.join(output_path, base_model_job, 'output', 'model.tar.gz')\n",
    "\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({\n",
    "    'train': metadata['train'],\n",
    "    'val' : metadata['val'],\n",
    "    'base' : base_model},\n",
    "    job_name=job_name,\n",
    "    wait = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have identified the optimal setting and hyperparameters and trained the final model, we can move onto [testing our model](./ENindoor67-LocalTesting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
