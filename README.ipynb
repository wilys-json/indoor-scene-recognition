{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MIT-Indoor-67 Dataset](http://web.mit.edu/torralba/www/indoor.html)\n",
    "\n",
    "The [MIT Indoor Scene 67 dataset](http://groups.csail.mit.edu/vision/LabelMe/NewImages/indoorCVPR_09.tar), developed in MIT, contains 67 labelled categories with 15620 images. Following a 80:20 split, a subset was splited into the [train](http://web.mit.edu/torralba/www/TrainImages.txt) and [test](http://web.mit.edu/torralba/www/TestImages.txt) sets with labels and each class contains the same number of train and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MITIndoor67](http://web.mit.edu/torralba/www/allIndoors.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indoor Scene Classification\n",
    "This project is original the capstone of Udacity's Machine Learning Engineer Nanodegree. The goal was to train an EfficientNet model to classify indoor scene images with the MIT Indoor 67 dataset.\n",
    "\n",
    "\n",
    "The classification model was developed on AWS SageMaker and Pytorch framework. You can find the steps and process of developing an EfficientNet model of scene classification with the following notebooks:\n",
    "\n",
    "#### [1.  Data Exploration](./notebooks/ENindoor67-Exploration.ipynb)\n",
    "#### [2. Data Preprocessing](./notebooks/ENindoor67-Preprocessing.ipynb)\n",
    "#### [3. Benchmark Model](./notebooks/ResNeXt101.ipynb)\n",
    "#### [4. EfficientNet Base Model](./notebooks/EfficientNets-Base.ipynb)\n",
    "#### [5. Fine-tuning and Hyperparameter Tuning by Bayesian Search](./notebooks/EfficientNets-HPO.ipynb)\n",
    "#### [6. Model testing](./notebooks/ENindoor67-LocalTesting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving MIT Indoor 67 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the dataset (.tar) and the subset split labels (.txt), run the following cell in a jupyter notebook (.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-28 23:31:55--  http://groups.csail.mit.edu/vision/LabelMe/NewImages/indoorCVPR_09.tar\n",
      "Resolving groups.csail.mit.edu (groups.csail.mit.edu)... 128.30.2.44\n",
      "Connecting to groups.csail.mit.edu (groups.csail.mit.edu)|128.30.2.44|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2592010240 (2.4G) [application/x-tar]\n",
      "Saving to: ‘indoorCVPR_09.tar’\n",
      "\n",
      "indoorCVPR_09.tar   100%[===================>]   2.41G  9.54MB/s    in 3m 27s  \n",
      "\n",
      "2020-11-28 23:35:23 (11.9 MB/s) - ‘indoorCVPR_09.tar’ saved [2592010240/2592010240]\n",
      "\n",
      "--2020-11-28 23:35:23--  http://web.mit.edu/torralba/www/TrainImages.txt\n",
      "Resolving web.mit.edu (web.mit.edu)... 104.100.30.13, 2600:1408:8400:58e::255e, 2600:1408:8400:5ab::255e\n",
      "Connecting to web.mit.edu (web.mit.edu)|104.100.30.13|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 172080 (168K) [text/plain]\n",
      "Saving to: ‘data/mit_indoor_67/TrainImages.txt.1’\n",
      "\n",
      "TrainImages.txt.1   100%[===================>] 168.05K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2020-11-28 23:35:23 (2.38 MB/s) - ‘data/mit_indoor_67/TrainImages.txt.1’ saved [172080/172080]\n",
      "\n",
      "--2020-11-28 23:35:23--  http://web.mit.edu/torralba/www/TestImages.txt\n",
      "Resolving web.mit.edu (web.mit.edu)... 104.100.30.13, 2600:1408:8400:5ab::255e, 2600:1408:8400:58e::255e\n",
      "Connecting to web.mit.edu (web.mit.edu)|104.100.30.13|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43475 (42K) [text/plain]\n",
      "Saving to: ‘data/mit_indoor_67/TestImages.txt.1’\n",
      "\n",
      "TestImages.txt.1    100%[===================>]  42.46K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2020-11-28 23:35:23 (2.83 MB/s) - ‘data/mit_indoor_67/TestImages.txt.1’ saved [43475/43475]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data/mit_indoor_67/raw\n",
    "!wget http://groups.csail.mit.edu/vision/LabelMe/NewImages/indoorCVPR_09.tar\n",
    "!wget http://web.mit.edu/torralba/www/TrainImages.txt -P data/mit_indoor_67\n",
    "!wget http://web.mit.edu/torralba/www/TestImages.txt -P data/mit_indoor_67\n",
    "!tar -xf indoorCVPR_09.tar -C data/mit_indoor_67/raw\n",
    "!rm -rf indoorCVPR_09.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have an overview of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 67\n",
      "Number of Images: 15620\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "data_dir = 'data/mit_indoor_67/raw/Images'\n",
    "category_dir = glob(os.path.join(data_dir, '*'))\n",
    "image_files = []\n",
    "for category in category_dir:\n",
    "    image_files += glob(os.path.join(category, '*'))\n",
    "    \n",
    "print(f\"\"\"Number of categories: {len(category_dir)}\n",
    "Number of Images: {len(image_files)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the train-test split in the [original paper](http://people.csail.mit.edu/torralba/publications/indoor.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET:\n",
      "Number of samples: 5360\n",
      "Number of categories: 67\n",
      "Subsample range of each category (min. - max.): 77 - 83\n",
      "TEST SET:\n",
      "Number of samples: 1340\n",
      "Number of categories: 67\n",
      "Subsample range of each category (min. - max.): 17 - 23\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from heapq import nlargest, nsmallest\n",
    "\n",
    "train_txt = 'data/mit_indoor_67/TrainImages.txt'\n",
    "test_txt = 'data/mit_indoor_67/TestImages.txt'\n",
    "\n",
    "train_image_paths = open(train_txt).read().split('\\n')\n",
    "test_image_paths = open(test_txt).read().split('\\n')\n",
    "\n",
    "train_image_counts = Counter([path.split('/')[0] for path in train_image_paths])\n",
    "test_image_counts = Counter([path.split('/')[0] for path in test_image_paths])\n",
    "\n",
    "print(f\"\"\"TRAINING SET:\n",
    "Number of samples: {len(train_image_paths)}\n",
    "Number of categories: {len(train_image_counts)}\n",
    "Subsample range of each category (min. - max.): {train_image_counts[nsmallest(1, train_image_counts, key=train_image_counts.get)[0]]} - {train_image_counts[nlargest(1, train_image_counts, key=train_image_counts.get)[0]]}\"\"\")\n",
    "\n",
    "print(f\"\"\"TEST SET:\n",
    "Number of samples: {len(test_image_paths)}\n",
    "Number of categories: {len(test_image_counts)}\n",
    "Subsample range of each category (min. - max.): {test_image_counts[nsmallest(1, test_image_counts, key=test_image_counts.get)[0]]} - {test_image_counts[nlargest(1, test_image_counts, key=test_image_counts.get)[0]]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original study used only a subset of the sample. In order to train our model with a larger dataset, we will follow the study's 80:20 split on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at the size of the first 10 images in both sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST 10 TRAIN IMAGE SIZE:\n",
      "\n",
      "1 - gameroom/bt_132294gameroom2.jpg is of size: 296 x 397\n",
      "2 - poolinside/inside_pool_and_hot_tub.jpg is of size: 412 x 550\n",
      "3 - winecellar/bodega_12_11_flickr.jpg is of size: 375 x 500\n",
      "4 - casino/casino_0512.jpg is of size: 258 x 400\n",
      "5 - livingroom/living58.jpg is of size: 768 x 1024\n",
      "6 - mall/4984307.jpg is of size: 1261 x 1280\n",
      "7 - corridor/pasilltmpo_t.jpg is of size: 256 x 257\n",
      "8 - laboratorywet/laboratorio_quimica_07_05_altavista.jpg is of size: 296 x 396\n",
      "9 - bookstore/CIMG2743.jpg is of size: 336 x 448\n",
      "10 - casino/casino_0044.jpg is of size: 266 x 400\n",
      "\n",
      "FIRST 10 TEST IMAGE SIZE:\n",
      "\n",
      "1 - kitchen/int474.jpg is of size: 256 x 256\n",
      "2 - operating_room/operating_room_31_03_altavista.jpg is of size: 209 x 260\n",
      "3 - restaurant_kitchen/restaurant_kitchen_google_0075.jpg is of size: 351 x 470\n",
      "4 - videostore/videoclub_05_14_flickr.jpg is of size: 500 x 375\n",
      "5 - poolinside/piscine_interieureee.jpg is of size: 369 x 460\n",
      "6 - videostore/blockbuster_08_10_flickr.jpg is of size: 415 x 500\n",
      "7 - poolinside/piscina_cubierta_07_19_altavista.jpg is of size: 225 x 300\n",
      "8 - mall/mall26.jpg is of size: 256 x 256\n",
      "9 - kindergarden/toddler.jpg is of size: 376 x 628\n",
      "10 - buffet/Buffet_1.jpg is of size: 503 x 668\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def print_shape(data_dir, paths):\n",
    "    print()\n",
    "    for idx, path in enumerate(paths):\n",
    "        img = cv2.imread(os.path.join(data_dir, path))\n",
    "        print(f'{idx+1} - {path} is of size: {img.shape[0]} x {img.shape[1]}')\n",
    "\n",
    "print(\"FIRST 10 TRAIN IMAGE SIZE:\")\n",
    "print_shape(data_dir, train_image_paths[:10])\n",
    "print()\n",
    "print(\"FIRST 10 TEST IMAGE SIZE:\")\n",
    "print_shape(data_dir, test_image_paths[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images seem to have various size. Hence, we may want to resize the images in our pre-processing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first of all [explore our data](notebooks/ENindoor67-Exploration.ipynb) for exploration, visualization and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
